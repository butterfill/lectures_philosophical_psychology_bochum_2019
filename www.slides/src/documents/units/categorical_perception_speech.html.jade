---cson
layout: 'deck_unit'
title: "Speech Perception"
tags: []
description: """
  """
---

include ../../../fragments/unit_mixins



+slide_middle
  p.center articulatory gesture
  .notes: :t
    In speaking we produce an overlapping sequence of articulatory gestures, which are motor actions
    involving coordinated movements of the lips, tongue, velum and larynx. These gestures are the units
    in terms of which we plan utterances (Browman and Goldstein 1992; Goldstein, Fowler, et al. 2003).
  .notes: :t
    These are the actions I want to focus on first in thinking about
    what we experience when we encounter others’ actions.
    
    
+slide
  +img_clip('browman_1996_fig1.png')
  p.source Browman & Goldstein 1986, figure 1
  .notes: :t
    ‘Trajectory of lower lip in [abe] as measured by tracking infra-redLED
    placed on subject's lower lip’
  .notes: :t
    ‘Not every utterance of word transcribed with /b/ will display exactly the trajectory
    of Fig. 1.: the trajectory will vary with vowel context, syllable position,
    stress, speaking rate and speaker.
    We must, therefore, ultimately characterise /b/ as a family of patterns of lip
    movement’
    \citep[p.~224]{browman:1986_towards}
    
//- distinct processes

+slide_middle
  p.center Speech and auditory perception involve distinct processes

+slide({bkg:'duplex_01.jpg'})
  .notes: :t
    A schematic spectrogram for a synthetic sound which is normally perceived as [ra]. The horizontal
    axis represents time, the vertical frequency.
  .notes: :t
    A schematic spectrogram for [la].  

+slide({bkg:'duplex_02.jpg'})
  .notes: :t
    In the middle you see the \emph{base}, i.e. the part of the spectrogram common to [ra] and [la]. This
    is played to one ear
  .notes: :t
    Below you see the transitions, , i.e. the parts of the spectrogram that differ between [ra] and [la].
    When played in isolation these sound like a chirp. When played at the same time as the base but in
    the other ear, subjects hear a chirp and a [ra] or a [la] depending on which transition is played.
  .notes: :t
    How do we know that the same stimuli may be processed by different perceptual systems
    concurrently—for instance, how do we know that speech and auditory processing are distinct? A
    phenomenon called “duplex” perception demonstrates their distinctness occurs in. Artificial
    speech-like stimuli for two syllables, [ra] and [la], are generated. The acoustic signals for each
    syllable is artificially broken up into two parts, the “base” and “transition” (see Fig. *** below).
    The syllables have the same “base” but differ in the “transition”. When the “transition” is played
    alone it sounds like a chirp and quite unlike anything we normally hear in speech. Duplex perception
    occurs when the base and transition are played together but in separate ears. In this case, subjects
    hear both the chirp that they hear when the transition is played in isolated, and the syllable [la]
    or [ra]. Which syllable they hear depends on which transition is played, so speech processing must
    have combined the base and transition. By contrast, auditory processing must have failed to combine
    them because otherwise the chirp would not have been heard. In this case, then, the perception
    resulting from the duplex presentation involves simultaneous auditory and speech recognition
    processes. This shows that auditory and speech processing are distinct perceptual processes.
  .notes: :t
    The duplex case is unusual. We can’t normally hear the chirps we make in speaking because speech
    processing inhibits this level of auditory processing. But plainly speech is subject to some auditory
    processing for we can hear extra-linguistic qualities of speech; some of these provide cues to
    emotional state, gender and class. Perception of these extra-linguistic qualities enables us to
    distinguish stimuli within a category. As already mentioned, this is a problem for Repp’s operational
    definition. Our ability to discriminate stimuli is the product of both categorical speech processing
    and non-categorical auditory processing. If we want to get at the essence of categorical perception
    it seems there is no alternative but to appeal to particular perceptual processes rather than
    behaviours.
    
  .notes Source: \citep{Liberman:1981xk}




//- categorical perception of speech exists
+slide({bkg:'categorical_perception_of_speech/Slide01.jpg'})
  .notes: :t
    Here are 12 speech-like sounds.
    Acoustically each differs from its neighbours no more than any other does.
    

+slide({bkg:'categorical_perception_of_speech/Slide02.jpg'})
  .notes: :t
    They would be labelled differently

+slide({bkg:'categorical_perception_of_speech/Slide03.jpg'})
  .notes: :t
    And within a label they are relatively hard to disciminate whereas ...

+slide({bkg:'categorical_perception_of_speech/Slide04.jpg'})
  .notes: :t
    Discriminating acoustically no less similar stimuli that are given
    different labels is easier (faster and more accurate).
  .notes: :t
    This is categorical perception: speed and accuracy maps onto labelling ...
    
  
+slide({bkg:'categorical_perception_of_speech/Slide06.jpg'})
  .notes: :t
    Categorical perception of mating calls and perhaps other acoustic signals is widespread in non-human
    animals including monkeys, mice and chinchillas (Ehret 1987; Kuhl 1987), and is even found in
    cognitively unsophisticated animals such as frogs (Baugh, Akre and Ryan 2008) and crickets
    (Wyttenbach, May and Hoy 1996).




//- WHAT ARE THE OBJECTS OF CATEGORICAL PERCEPTION

+slide_middle
  p.center What are the objects of categorical perception?

+slide({bkg:'categorical_perception_of_speech/Slide26.jpg'})
  .notes: :t
    the location of the category boundaries changes depending on contextual factors such as the
    speaker’s dialect,22 or the rate at which the speaker talks;23 both factors dramatically affect
    which sounds are produced.
+slide({bkg:'categorical_perception_of_speech/Slide27.jpg'})
  .notes: :t
    This means that in two different contexts, different stimuli may result in the same perceptions, and
    the same stimulus may result in different perceptions.
+slide({bkg:'categorical_perception_of_speech/Slide23.jpg'})
+slide({bkg:'categorical_perception_of_speech/Slide24.jpg'})
+slide({bkg:'categorical_perception_of_speech/Slide25.jpg'})
  .notes: :t
    co-articulation, the fact that phonic gestures overlap (this is what makes talking fast).

+slide_middle
  p.center What are the objects of categorical perception?
  .notes.show.em-above
    p.em-around 1. Speech perception is categorical
    p.em-around   2. The category boundaries correspond (imperfectly but robustly) to differences in articulatory gestures
    p.em-around   3.  The best explanation of (2) involves the hypothesis that the objects of speech perception are articulatory gestures
  .notes: :t
    \emph{Articulatory Gesture:}
    In speaking we produce an overlapping sequence of articulatory gestures, which are motor actions
    involving coordinated movements of the lips, tongue, velum and larynx. These gestures are the units
    in terms of which we plan utterances (Browman and Goldstein 1992; Goldstein, Fowler, et al. 2003).
    



